{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c343ab",
   "metadata": {},
   "source": [
    "# üé® Clothing Classifier Training Notebook\n",
    "\n",
    "This notebook trains a clothing classification model using **MobileNetV2** transfer learning.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Base: MobileNetV2 (ImageNet pretrained)\n",
    "- Custom top: Dense layers with dropout\n",
    "- Output: 6 clothing categories\n",
    "\n",
    "**Training Strategy:**\n",
    "- Phase 1: Frozen base (15 epochs)\n",
    "- Phase 2: Fine-tuning last 20 layers (15 epochs)\n",
    "\n",
    "**Expected Performance:** 95%+ accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd650fee",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bcb11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "TensorFlow version: 2.10.0\n",
      "Keras version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Suppress TensorFlow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eac1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  No GPU detected, using CPU\n",
      "   Training will be slower but still functional\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"üéÆ GPU detected: {gpus[0].name}\")\n",
    "    print(f\"   Memory growth enabled\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"   ‚úÖ GPU will be used for training (6x faster)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected, using CPU\")\n",
    "    print(\"   Training will be slower but still functional\")\n",
    "    print()\n",
    "    print(\"üìå You have RTX 3050 GPU but TensorFlow can't detect it.\")\n",
    "    print(\"   This is NORMAL - you only have Python GPU packages, not CUDA Toolkit.\")\n",
    "    print()\n",
    "    print(\"‚úÖ OPTION 1: Continue with CPU (Recommended for now)\")\n",
    "    print(\"   - Training will work perfectly fine\")\n",
    "    print(\"   - Just takes longer (~2-3 hours vs 30-40 minutes)\")\n",
    "    print(\"   - No installation needed\")\n",
    "    print()\n",
    "    print(\"üöÄ OPTION 2: Enable GPU (Optional - for 6x speedup)\")\n",
    "    print(\"   Step 1: Download CUDA Toolkit 12.6\")\n",
    "    print(\"           https://developer.nvidia.com/cuda-downloads\")\n",
    "    print(\"   Step 2: Install with default settings\")\n",
    "    print(\"   Step 3: Restart Jupyter notebook\")\n",
    "    print(\"   Step 4: Re-run this cell - GPU will be detected\")\n",
    "    print()\n",
    "    print(\"üí° For this tutorial, CPU mode is fine. You can enable GPU later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7949e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify compute device and estimate training time\n",
    "device_name = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üñ•Ô∏è  COMPUTE DEVICE: {device_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if device_name == \"CPU\":\n",
    "    print(\"‚è±Ô∏è  Estimated training time:\")\n",
    "    print(\"   - Phase 1 (15 epochs): ~60-90 minutes\")\n",
    "    print(\"   - Phase 2 (15 epochs): ~60-90 minutes\")\n",
    "    print(\"   - Total: ~2-3 hours\")\n",
    "    print()\n",
    "    print(\"üí° TIP: You can reduce epochs for faster testing:\")\n",
    "    print(\"   EPOCHS1 = 5  # Instead of 15\")\n",
    "    print(\"   EPOCHS2 = 5  # Instead of 15\")\n",
    "else:\n",
    "    print(\"‚è±Ô∏è  Estimated training time:\")\n",
    "    print(\"   - Phase 1 (15 epochs): ~15-20 minutes\")\n",
    "    print(\"   - Phase 2 (15 epochs): ~15-20 minutes\")\n",
    "    print(\"   - Total: ~30-40 minutes\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(\"‚úÖ Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1283c2e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update paths for Windows\n",
    "ROOT = Path(r'C:\\Users\\Prachi\\Desktop\\qq\\AIProject')\n",
    "DATA = ROOT / 'data'\n",
    "PROCESSED = DATA / 'processed'\n",
    "RAW = DATA / 'raw'\n",
    "MODELS = ROOT / 'models' / 'saved_models'\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS1 = 15  # Phase 1: Frozen base\n",
    "EPOCHS2 = 15  # Phase 2: Fine-tuning\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ CLOTHING CLASSIFIER TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Data directory: {PROCESSED}\")\n",
    "print(f\"Models directory: {MODELS}\")\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686a2c0",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "train_df = pd.read_csv(PROCESSED / 'train.csv')\n",
    "val_df = pd.read_csv(PROCESSED / 'val.csv')\n",
    "test_df = pd.read_csv(PROCESSED / 'test.csv')\n",
    "\n",
    "# Load label mapping\n",
    "with open(PROCESSED / 'label_mapping.json') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Train samples: {len(train_df):,}\")\n",
    "print(f\"   Validation samples: {len(val_df):,}\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "print(f\"   Total classes: {num_classes}\")\n",
    "print(f\"\\nüìã Classes: {list(label_mapping.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Training set distribution\n",
    "train_counts = train_df['main_category'].value_counts()\n",
    "axes[0].bar(train_counts.index, train_counts.values, color='skyblue')\n",
    "axes[0].set_title('Training Set Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(train_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Validation set distribution\n",
    "val_counts = val_df['main_category'].value_counts()\n",
    "axes[1].bar(val_counts.index, val_counts.values, color='lightcoral')\n",
    "axes[1].set_title('Validation Set Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(val_counts.values):\n",
    "    axes[1].text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Test set distribution\n",
    "test_counts = test_df['main_category'].value_counts()\n",
    "axes[2].bar(test_counts.index, test_counts.values, color='lightgreen')\n",
    "axes[2].set_title('Test Set Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Category')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(test_counts.values):\n",
    "    axes[2].text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Class distributions visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2442b9a",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Create TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, training=True):\n",
    "    \"\"\"\n",
    "    Create TensorFlow dataset from DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'id' and 'category_label' columns\n",
    "        training: If True, apply data augmentation\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset\n",
    "    \"\"\"\n",
    "    def load_img(path, label):\n",
    "        # Read and decode image\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, 0.2)\n",
    "            img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    # Create file paths\n",
    "    paths = [str(RAW / 'images' / f\"{row['id']}.jpg\") for _, row in df.iterrows()]\n",
    "    labels = df['category_label'].values\n",
    "    \n",
    "    # Create dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    ds = ds.map(load_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    \n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "print(\"üìä Creating TensorFlow datasets...\")\n",
    "train_ds = make_dataset(train_df, training=True)\n",
    "val_ds = make_dataset(val_df, training=False)\n",
    "test_ds = make_dataset(test_df, training=False)\n",
    "\n",
    "print(\"‚úÖ Datasets created successfully!\")\n",
    "print(f\"   Training batches: ~{len(train_df) // BATCH_SIZE}\")\n",
    "print(f\"   Validation batches: ~{len(val_df) // BATCH_SIZE}\")\n",
    "print(f\"   Test batches: ~{len(test_df) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from training set\n",
    "sample_batch = next(iter(train_ds))\n",
    "sample_images, sample_labels = sample_batch\n",
    "\n",
    "# Reverse label mapping\n",
    "reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(sample_images[i].numpy())\n",
    "    label_name = reverse_mapping[int(sample_labels[i])]\n",
    "    axes[i].set_title(f'{label_name}', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Training Images with Augmentation', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample images visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b772de",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è  Building model architecture...\")\n",
    "\n",
    "# Load pre-trained MobileNetV2\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "base_model.trainable = False  # Freeze for Phase 1\n",
    "\n",
    "# Build custom classifier on top\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "], name='ClothingClassifier')\n",
    "\n",
    "print(f\"‚úÖ Model built successfully!\")\n",
    "print(f\"   Total parameters: {model.count_params():,}\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "print(f\"   Non-trainable parameters: {sum([tf.size(w).numpy() for w in model.non_trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b17d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(\n",
    "    model,\n",
    "    to_file='clothing_classifier_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model architecture diagram saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e028975",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Phase 1: Train with Frozen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a067348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: Training with Frozen Base\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Only training custom top layers while base model is frozen\")\n",
    "print(f\"Learning rate: 0.001\")\n",
    "print(f\"Epochs: {EPOCHS1}\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_phase1 = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS / 'classifier_phase1_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Starting Phase 1 training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase 1\n",
    "history_phase1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS1,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Phase 1 training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history_phase1.history['accuracy'], label='Training', linewidth=2, marker='o')\n",
    "axes[0].plot(history_phase1.history['val_accuracy'], label='Validation', linewidth=2, marker='s')\n",
    "axes[0].set_title('Phase 1: Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history_phase1.history['loss'], label='Training', linewidth=2, marker='o')\n",
    "axes[1].plot(history_phase1.history['val_loss'], label='Validation', linewidth=2, marker='s')\n",
    "axes[1].set_title('Phase 1: Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Phase 1 Best Validation Accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d72af",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Phase 2: Fine-tuning with Unfrozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: Fine-tuning\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Unfreezing last 20 layers of base model for fine-tuning\")\n",
    "\n",
    "# Unfreeze base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except last 20\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_layers = sum([layer.trainable for layer in base_model.layers])\n",
    "print(f\"\\nTrainable layers in base model: {trainable_layers}/{len(base_model.layers)}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),  # Much lower LR\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: 0.00001 (10x lower than Phase 1)\")\n",
    "print(f\"Epochs: {EPOCHS2}\")\n",
    "\n",
    "# Callbacks for Phase 2\n",
    "callbacks_phase2 = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS / 'classifier_phase2_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Starting Phase 2 training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase 2\n",
    "history_phase2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS2,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35362166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Phase 2 training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history_phase2.history['accuracy'], label='Training', linewidth=2, marker='o', color='green')\n",
    "axes[0].plot(history_phase2.history['val_accuracy'], label='Validation', linewidth=2, marker='s', color='orange')\n",
    "axes[0].set_title('Phase 2: Model Accuracy (Fine-tuning)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history_phase2.history['loss'], label='Training', linewidth=2, marker='o', color='green')\n",
    "axes[1].plot(history_phase2.history['val_loss'], label='Validation', linewidth=2, marker='s', color='orange')\n",
    "axes[1].set_title('Phase 2: Model Loss (Fine-tuning)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Phase 2 Best Validation Accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both phases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Combined accuracy\n",
    "phase1_acc = history_phase1.history['val_accuracy']\n",
    "phase2_acc = history_phase2.history['val_accuracy']\n",
    "combined_acc = phase1_acc + phase2_acc\n",
    "\n",
    "axes[0].plot(range(1, len(combined_acc) + 1), combined_acc, linewidth=2, marker='o', color='purple')\n",
    "axes[0].axvline(x=len(phase1_acc), color='red', linestyle='--', linewidth=2, label='Phase 1 ‚Üí Phase 2')\n",
    "axes[0].set_title('Combined Training: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Combined loss\n",
    "phase1_loss = history_phase1.history['val_loss']\n",
    "phase2_loss = history_phase2.history['val_loss']\n",
    "combined_loss = phase1_loss + phase2_loss\n",
    "\n",
    "axes[1].plot(range(1, len(combined_loss) + 1), combined_loss, linewidth=2, marker='s', color='brown')\n",
    "axes[1].axvline(x=len(phase1_loss), color='red', linestyle='--', linewidth=2, label='Phase 1 ‚Üí Phase 2')\n",
    "axes[1].set_title('Combined Training: Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893c7b",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate\n",
    "test_results = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"   Loss: {test_results['loss']:.4f}\")\n",
    "print(f\"   Accuracy: {test_results['accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for confusion matrix\n",
    "print(\"\\nüìä Generating predictions for test set...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(pred_classes)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(y_true)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = list(label_mapping.keys())\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(report_df.to_string())\n",
    "\n",
    "# Visualize per-class metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_df = report_df.iloc[:-3, :3]  # Exclude avg rows\n",
    "\n",
    "# Precision\n",
    "axes[0].barh(metrics_df.index, metrics_df['precision'], color='skyblue')\n",
    "axes[0].set_xlabel('Precision', fontweight='bold')\n",
    "axes[0].set_title('Per-Class Precision', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1].barh(metrics_df.index, metrics_df['recall'], color='lightcoral')\n",
    "axes[1].set_xlabel('Recall', fontweight='bold')\n",
    "axes[1].set_title('Per-Class Recall', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "axes[2].barh(metrics_df.index, metrics_df['f1-score'], color='lightgreen')\n",
    "axes[2].set_xlabel('F1-Score', fontweight='bold')\n",
    "axes[2].set_title('Per-Class F1-Score', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlim([0, 1])\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c7ee1",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = MODELS / 'clothing_classifier.keras'\n",
    "model.save(final_model_path)\n",
    "\n",
    "print(f\"\\nüíæ Final model saved: {final_model_path}\")\n",
    "print(f\"   Model size: {final_model_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Save training history\n",
    "history_dict = {\n",
    "    'phase1': {\n",
    "        'loss': [float(x) for x in history_phase1.history['loss']],\n",
    "        'accuracy': [float(x) for x in history_phase1.history['accuracy']],\n",
    "        'val_loss': [float(x) for x in history_phase1.history['val_loss']],\n",
    "        'val_accuracy': [float(x) for x in history_phase1.history['val_accuracy']]\n",
    "    },\n",
    "    'phase2': {\n",
    "        'loss': [float(x) for x in history_phase2.history['loss']],\n",
    "        'accuracy': [float(x) for x in history_phase2.history['accuracy']],\n",
    "        'val_loss': [float(x) for x in history_phase2.history['val_loss']],\n",
    "        'val_accuracy': [float(x) for x in history_phase2.history['val_accuracy']]\n",
    "    },\n",
    "    'test': {\n",
    "        'loss': float(test_results['loss']),\n",
    "        'accuracy': float(test_results['accuracy'])\n",
    "    }\n",
    "}\n",
    "\n",
    "history_path = MODELS / 'history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Training history saved: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d396b7f",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Summary:\n",
    "- ‚úÖ Model trained with 2-phase approach\n",
    "- ‚úÖ Final test accuracy: **{:.2f}%**\n",
    "- ‚úÖ Model saved and ready for inference\n",
    "- ‚úÖ Complete training history saved\n",
    "\n",
    "### Next Steps:\n",
    "1. Use the model in `backend/main.py` for API inference\n",
    "2. Test the model with real clothing images\n",
    "3. Fine-tune further if needed with more data\n",
    "4. Deploy to production environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c355536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nFinal Test Accuracy: {test_results['accuracy']*100:.2f}%\")\n",
    "print(f\"Model saved to: {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
